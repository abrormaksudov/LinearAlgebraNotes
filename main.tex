\documentclass[a4paper,11pt]{article}

\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{hyperref}
\usepackage{datetime2}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{tcolorbox}
\tcbuselibrary{breakable, skins}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{centernot}


\newcommand{\startdate}{November 8, 2025}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


\title{Linear Algebra Notes}
\author{Abror Maksudov}
\date{Created: \startdate \\[7pt] Last Updated: \DTMnow}

\everymath{\displaystyle}

\begin{document}

\maketitle
\tableofcontents



\section{Vectors}



\subsection{Definition of a Scalar}
\begin{tcolorbox}
    A scalar is a \textbf{single number}, denoted as $a, b, c, \alpha, \beta, \gamma, \lambda, \mu$.
    
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}:  $N/A$.
\end{tcolorbox}



\subsection{Definition of a Vector}
\begin{tcolorbox}
    A vector is an \textbf{ordered list} of numbers, usually denoted as $\mathbf{v}$ or $\vec{v}$.

    \begin{itemize}
        \item \textbf{Column Vector (Default/Standard):}
        $$
        \vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix} = \begin{bmatrix} v_1 & v_2 & \cdots & v_n \end{bmatrix}^T
        $$

        \item \textbf{Row Vector:}
        $$
        \vec{v} =  \begin{bmatrix} v_1 & v_2 & \cdots & v_n \end{bmatrix} = \begin{pmatrix} v_1 & v_2 & \cdots & v_n \end{pmatrix} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}^T
        $$
    \end{itemize}

    \tcblower
    \textit{Space Complexity}: $O(n)$ | \textit{Time Complexity}:  $N/A$.
\end{tcolorbox}



\subsection{Vector Operations}
\begin{tcolorbox}
    For vectors $\vec{u}, \vec{v} \in \mathbb{R}^n$ and scalar $c \in \mathbb{R}$, the following rules hold:
    \begin{enumerate}
        \item \textbf{Scalar Multiplication:}
        $$
        c\vec{v} = c\begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = \begin{bmatrix} cv_1 \\ cv_2 \\ \vdots \\ cv_n \end{bmatrix}
        $$
        
        \item \textbf{Vector Addition:}
        $$
        \vec{u} + \vec{v} = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix} + \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix}
        $$
        \emph{Requirement:} Both vectors must have the same dimension.
        
        \item \textbf{Vector Subtraction:}
        $$
        \vec{u} - \vec{v} = \vec{u} + (-1)\vec{v} = \begin{bmatrix} u_1 - v_1 \\ u_2 - v_2 \\ \vdots \\ u_n - v_n \end{bmatrix}
        $$
        \emph{Requirement:} Both vectors must have the same dimension.
    \end{enumerate}
    
    \tcblower
    \textit{Space Complexity}: $O(n)$ | \textit{Time Complexity}:  $O(n)$.
\end{tcolorbox}



\subsection{Properties of Vector}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v}, \vec{w} \in \mathbb{R}^n$ and $c, d \in \mathbb{R}$. Then:
    \[
    \begin{aligned}
        &\text{(1)} \quad \vec{u} + \vec{v} = \vec{v} + \vec{u} && \text{(Commutativity)} \\[8pt]
        &\text{(2)} \quad (\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w}) && \text{(Associativity)} \\[8pt]
        &\text{(3)} \quad \vec{v} + \vec{0} = \vec{v} && \text{(Identity)} \\[8pt]
        &\text{(4)} \quad \vec{v} + (-\vec{v}) = \vec{0} && \text{(Inverse)} \\[8pt]
        &\text{(5)} \quad c(d\vec{v}) = (cd)\vec{v} && \text{(Associativity of scalars)} \\[8pt]
        &\text{(6)} \quad 1 \cdot \vec{v} = \vec{v} && \text{(Multiplicative identity)} \\[8pt]
        &\text{(7)} \quad c(\vec{u} + \vec{v}) = c\vec{u} + c\vec{v} && \text{(Distributivity over vectors)} \\[8pt]
        &\text{(8)} \quad (c + d)\vec{v} = c\vec{v} + d\vec{v} && \text{(Distributivity over scalars)}
    \end{aligned}
    \]
    These 8 axioms define a linear space (or vector space) in $\mathbb{R}^n.$
\end{tcolorbox}



\subsection{Linear Combinations}
\begin{tcolorbox}
    A vector $\vec{w} \in \mathbb{R}^n$ is a \textbf{linear combination} of vectors $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k \in \mathbb{R}^n$ if there exist scalars $c_1, c_2, \ldots, c_k \in \mathbb{R}$ such that:
    \[
    \vec{w} = c_1\vec{v}_1 + c_2\vec{v}_2 + \cdots + c_k\vec{v}_k = \sum_{i=1}^{k} c_i\vec{v}_i = \begin{bmatrix} | & | & & | \\ \vec{v}_1 & \vec{v}_2 & \cdots & \vec{v}_k \\ | & | & & | \end{bmatrix} \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_k \end{bmatrix} = \vec{w}
    \]
    
    The scalars $c_1, c_2, \ldots, c_k$ are called \textbf{coefficients} or \textbf{weights} of the linear combination.
    
    \tcblower
    \textit{Space Complexity}: $O(n)$ | \textit{Time Complexity}: $O(nk)$.
\end{tcolorbox}



\subsection{Dot Product}

\begin{tcolorbox}
    The \textbf{dot product} of two vectors $\vec{u}, \vec{v} \in \mathbb{R}^n$ is a scalar defined as:
    \[
    \vec{u} \cdot \vec{v} = u_1v_1 + u_2v_2 + \cdots + u_nv_n = \sum_{i=1}^{n} u_iv_i
    \]

    \textbf{Geometric View:}
    \[
    \vec{u} \cdot \vec{v} = \|\vec{u}\| \|\vec{v}\| \cos\theta
    \]
    where $\theta$ is the angle between $\vec{u}$ and $\vec{v}$, and $\|\vec{u}\|$ is the \textbf{magnitude} (or \textbf{length} or \textbf{norm}) of $\vec{u}$. \\
    
    \textbf{Alternative notations:}
    \[
    \vec{u} \cdot \vec{v} = \langle \vec{u}, \vec{v} \rangle = \mathbf{u}^T\mathbf{v}
    \]
    
    \textbf{Special Cases:}
    \[
    \begin{aligned}
        &\text{(1)} \quad \vec{u} \cdot \vec{v} = 0 \iff \theta = 90^\circ && \text{(Perpendicular)} \\[8pt]
        &\text{(2)} \quad \vec{u} \cdot \vec{v} > 0 \iff \theta < 90^\circ && \text{(Acute angle)} \\[8pt]
        &\text{(3)} \quad \vec{u} \cdot \vec{v} < 0 \iff \theta > 90^\circ && \text{(Obtuse angle)} \\[8pt]
        &\text{(4)} \quad \vec{u} \cdot \vec{u} = \|\vec{u}\|^2 \iff \theta = 0^\circ && \text{(Squared magnitude)} \\[8pt]
    \end{aligned}
    \]
    
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$. \\
    \textit{Note:} Highly optimized using SIMD (Single Instruction, Multiple Data) operations.
\end{tcolorbox}



\subsection{Properties of the Dot Product}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v}, \vec{w} \in \mathbb{R}^n$ and $c \in \mathbb{R}$. Then:
    \[
    \begin{aligned}
        &\text{(1)} \quad \vec{u} \cdot \vec{v} = \vec{v} \cdot \vec{u} && \text{(Commutativity)} \\[8pt]
        &\text{(2)} \quad \vec{u} \cdot (\vec{v} + \vec{w}) = \vec{u} \cdot \vec{v} + \vec{u} \cdot \vec{w} && \text{(Distributivity)} \\[8pt]
        &\text{(3)} \quad (c\vec{u}) \cdot \vec{v} = c(\vec{u} \cdot \vec{v}) = \vec{u} \cdot (c\vec{v}) && \text{(Scalar multiplication)} \\[8pt]
        &\text{(4)} \quad \vec{u} \cdot \vec{u} \geq 0 && \text{(Positive definiteness)} \\[8pt]
        &\text{(5)} \quad \vec{u} \cdot \vec{u} = 0 \iff \vec{u} = \vec{0} && \text{(Definiteness)} \\[8pt]
        &\text{(6)} \quad \vec{0} \cdot \vec{v} = 0 && \text{(Zero vector)} \\[8pt]
    \end{aligned}
    \]

    These properties define the dot product as an \textbf{inner product} on $\mathbb{R}^n$.
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$. \\
\end{tcolorbox}



\subsection{Cross Product}
\begin{tcolorbox}
    The \textbf{cross product} of two vectors $\vec{u}, \vec{v} \in \mathbb{R}^3$ is a vector $\vec{u} \times \vec{v} \in \mathbb{R}^3$ that is \textbf{perpendicular} to both $\vec{u}$ and $\vec{v}$ as is defined as:
    \[
    \vec{u} \times \vec{v} = \begin{bmatrix} u_2v_3 - u_3v_2 \\ u_3v_1 - u_1v_3 \\ u_1v_2 - u_2v_1 \end{bmatrix}
    \]
    
    \tcblower
    \textit{Note:} Cross product is only defined in $\mathbb{R}^3$ (and $\mathbb{R}^7$, but rarely used). \\
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(1)$
\end{tcolorbox}



\subsection{Properties of the Cross Product}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v}, \vec{w} \in \mathbb{R}^3$, $c \in \mathbb{R}$ and $\theta$ be the angle between $\vec{u}$ and $\vec{v}$. Then:
    \[
    \begin{aligned}
        &\text{(1)} \quad \vec{u} \times \vec{v} = -(\vec{v} \times \vec{u}) && \text{(Anti-commutativity)} \\[8pt]
        &\text{(2)} \quad \vec{u} \times (\vec{v} + \vec{w}) = \vec{u} \times \vec{v} + \vec{u} \times \vec{w} && \text{(Distributivity)} \\[8pt]
        &\text{(3)} \quad (c\vec{u}) \times \vec{v} = c(\vec{u} \times \vec{v}) = \vec{u} \times (c\vec{v}) && \text{(Scalar multiplication)} \\[8pt]
        &\text{(4)} \quad \vec{u} \times \vec{u} = \vec{0} && \text{(Self cross product)} \\[8pt]
        &\text{(5)} \quad \vec{u} \times \vec{0} = \vec{0} \times \vec{u} = \vec{0} && \text{(Zero vector)} \\[8pt]
        &\text{(6)} \quad \vec{u} \times \vec{v} = \vec{0} \iff \vec{u} \parallel \vec{v} && \text{(Parallel vectors)} \\[8pt]
        &\text{(7)} \quad \vec{u} \cdot (\vec{u} \times \vec{v}) = 0, \quad \vec{v} \cdot (\vec{u} \times \vec{v}) = 0 && \text{(Orthogonality)} \\[8pt]
        &\text{(8)} \quad \|\vec{u} \times \vec{v}\| = \|\vec{u}\| \|\vec{v}\| \sin\theta && \text{(Magnitude)} \\[8pt]
    \end{aligned}
    \]

    \textbf{Warning:} The cross product is \textbf{not associative} in general: $\vec{u} \times (\vec{v} \times \vec{w}) \neq (\vec{u} \times \vec{v}) \times \vec{w}$.
    
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(1)$
\end{tcolorbox}



\subsection{Scalar Projection}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v} \in\mathbb{R}^n$. Then, the \textbf{scalar (component) projection} of $\vec{u}$ onto $\vec{v}$ is the signed length of the projection:

    \[
    \text{comp}_{\vec{v}} \vec{u} = \frac{\vec{u} \cdot \vec{v}}{\|\vec{v}\|}
    \]

    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$.
\end{tcolorbox}




\subsection{Vector Projection}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v} \in\mathbb{R}^n$. Then, the \textbf{vector projection} of $\vec{u}$ onto $\vec{v}$ is a vector in the direction of $\vec{v}$:
    \[
    \text{proj}_{\vec{v}} \vec{u} = \frac{\vec{u} \cdot \vec{v}}{\|\vec{v}\|^2} \vec{v} = \frac{\vec{u} \cdot \vec{v}}{\vec{v} \cdot \vec{v}} \vec{v} = \left(\text{comp}_{\vec{v}} \vec{u}\right) \frac{\vec{v}}{\|\vec{v}\|}
    \]

    \textbf{Orthogonal Component:}
    
    The component of $\vec{u}$ orthogonal to $\vec{v}$ is:
    \[
    \vec{u}_{\perp} = \vec{u} - \text{proj}_{\vec{v}} \vec{u}
    \]
    
    Note that $\vec{u} = \text{proj}_{\vec{v}} \vec{u} + \vec{u}_{\perp}$ (orthogonal decomposition).
    
    \tcblower
    \textit{Space Complexity}: $O(n)$ | \textit{Time Complexity}: $O(n)$.
\end{tcolorbox}



\subsection{Angle Between Vectors}
\begin{tcolorbox}
    The \textbf{angle} $\theta$ between two non-zero vectors $\vec{u}, \vec{v} \in \mathbb{R}^n$ is given by:
    \[
    \cos\theta = \frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\| \|\vec{v}\|} \iff     \theta = \arccos\left(\frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\| \|\vec{v}\|}\right), \quad 0 \leq \theta \leq \pi
    \]

    \textbf{Special Cases:}
    \[
    \begin{aligned}
        &\text{(1)} \quad \theta = 0 \iff \vec{u} \text{ and } \vec{v} \text{ point in the same direction} && (\cos\theta = 1) \\[8pt]
        &\text{(2)} \quad \theta = \frac{\pi}{2} \iff \vec{u} \perp \vec{v} && (\cos\theta = 0) \\[8pt]
        &\text{(3)} \quad \theta = \pi \iff \vec{u} \text{ and } \vec{v} \text{ point in opposite directions} && (\cos\theta = -1) \\[8pt]
    \end{aligned}
    \]
    
    \tcblower
    \textit{Note:} The formula $\vec{u} \cdot \vec{v} = \|\vec{u}\| \|\vec{v}\| \cos\theta$ is the geometric definition of the dot product. \\
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$
\end{tcolorbox}



\subsection{Cauchy-Schwarz Inequality}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v} \in \mathbb{R}^n$. Then:
    \[
    |\langle \vec{u}, \vec{v}\rangle|^2 \leq \langle\vec{u}, \vec{u}\rangle \cdot \langle\vec{v}, \vec{v}\rangle,
    \]
    or,
    \[
    |\langle \vec{u}, \vec{v}\rangle|^2 \leq \norm{\vec{u}}^2 \norm{\vec{v}}^2 \iff |\langle \vec{u}, \vec{v}\rangle| =\norm{\vec{u}} \norm{\vec{v}} |\cos\theta| \leq \norm{\vec{u}} \norm{\vec{v}},
    \]
    or in component form,
    \[
    \left(\sum_{i=1}^{n} u_iv_i\right)^2 \leq \left(\sum_{i=1}^{n} u_i^2\right)\left(\sum_{i=1}^{n} v_i^2\right).
    \]
    Equality holds if and only if $\vec{u}$ and $\vec{v}$ are linearly dependent.
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$
\end{tcolorbox}



\subsection{Triangle Inequality}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v} \in \mathbb{R}^n$. Then:
    \[
    \|\vec{u} + \vec{v}\| \leq \|\vec{u}\| + \|\vec{v}\|
    \]
    Equality holds if and only if $\vec{u} = c\vec{v}$ with $c \geq 0$. \\
    \textbf{Reverse Triangle Inequality:}
    \[
    \big| \|\vec{u}\| - \|\vec{v}\| \big| \leq \|\vec{u} - \vec{v}\|
    \]
    
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$
\end{tcolorbox}



\subsection{Vectorization for Computational Efficiency}
\begin{tcolorbox}
    \textbf{Vectorization} is the process of replacing explicit loops with vector/matrix operations to achieve dramatic performance improvements by leveraging optimized, low-level linear algebra libraries and hardware parallelism. \\
    
    \textbf{Performance Example:}
    \begin{center}
    \begin{tabular}{l|c|c|c}
        \hline
        \textbf{Operation} & \textbf{Complexity} & \textbf{Loop (Python)} & \textbf{Vectorized (NumPy)} \\
        \hline
        Dot Product ($n=10^6$) & $O(n)$ & $\sim 500$ ms & $\sim 1$ ms \\
        Matrix-Vector ($n \times n$) & $O(n^2)$ & $\sim 2000$ ms & $\sim 10$ ms \\
        Matrix-Matrix ($n \times n$) & $O(n^3)$ & $\sim$ hours & $\sim$ seconds \\
        \hline
    \end{tabular}
    \end{center}
    
    \textbf{Why Vectorization is Faster:}
    \begin{enumerate}
        \item \textbf{Hardware (SIMD):} Modern CPUs use \textbf{S}ingle \textbf{I}nstruction, \textbf{M}ultiple \textbf{D}ata (SIMD) instructions. This allows a single CPU instruction to perform the same operation (e.g., multiplication) on multiple data elements (e.g., 8 or 16 numbers) at the same time. Vectorized code uses these instructions automatically; Python loops do not.

        \item \textbf{Cache Efficiency:} Vectorized operations access memory in large, contiguous blocks. This is very "cache-friendly" and avoids the high cost of fetching data from main memory (RAM) for each small step of a loop.
        
        \item \textbf{Optimized Libraries:} NumPy, PyTorch, and TensorFlow operations are thin wrappers around highly optimized libraries (like BLAS/LAPACK, MKL, cuBLAS) written in C or Fortran. These libraries have been fine-tuned for decades.
    \end{enumerate}

    \textbf{Space-Time Trade-off:}
    \begin{itemize}
        \item Loops: $O(1)$ extra space, slow
        \item Vectorized: May need $O(n)$ temporary arrays, fast
        \item \textit{Rule:} Memory is cheap, time is expensive
    \end{itemize}
\end{tcolorbox}



\section{Matrices}



\subsection{Definition of a Matrix}
\begin{tcolorbox}[breakable]
    A matrix is a \textbf{rectangular array} of numbers arranged in rows and columns, denoted as $A, B, C$ or $\mathbf{A}, \mathbf{B}, \mathbf{C}$. \\
    
    An $m \times n$ matrix $A$ has $m$ rows and $n$ columns:
    \[
    A = 
    \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix}
    =
    \begin{pmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix} 
    = [a_{ij}]_{m \times n}
    \]
    
    where $a_{ij}$ is the element in the $i$-th row and $j$-th column also denoted as $A_{ij}$. \\

    \textbf{Notation:}
    \begin{itemize}
        \item $A \in \mathbb{R}^{m \times n}$ means $A$ is an $m \times n$ matrix with real entries
        \item $(A)_{ij} = a_{ij}$ denotes the $(i,j)$-entry of matrix $A$
        \item $m \times n$ is the \textbf{dimension} or \textbf{size} of the matrix
    \end{itemize}
    
    \textbf{Special Cases:}
    \[
    \begin{aligned}
        &\text{(1)} \quad m = n \implies A \text{ is a \textbf{square matrix}} \quad (n \times n) \\[8pt]
        &\text{(2)} \quad m = 1 \implies A \text{ is a \textbf{row vector}} \quad (1 \times n) \\[8pt]
        &\text{(3)} \quad n = 1 \implies A \text{ is a \textbf{column vector}} \quad (m \times 1) \\[8pt]
        &\text{(4)} \quad m = n = 1 \implies A \text{ is a \textbf{scalar}} \quad (1 \times 1)\\[8pt]
    \end{aligned}
    \]

    \textbf{Interpretations:}
    Unlike vectors, matrices have multiple common interpretations:
    
    \begin{enumerate}
        \item \textbf{As Data:} A way to store data in a grid.
        
        \item \textbf{As a Collection of Vectors:}
        \[
        \begin{array}{c@{\hspace{2cm}}c}
        \begin{bmatrix}
        | & | & & | \\
        \vec{c}_1 & \vec{c}_2 & \cdots & \vec{c}_n \\
        | & | & & |
        \end{bmatrix}
        &
        \begin{bmatrix}
        - & \vec{r}_1 & - \\
        - & \vec{r}_2 & - \\
        & \vdots & \\
        - & \vec{r}_m & -
        \end{bmatrix}
        \\
        \rule{0pt}{15pt}\text{Set of column vectors} &
        \rule{0pt}{15pt}\text{Set of row vectors}
        \end{array}
        \]
        
        \item \textbf{As a Linear Transformation:} A function that "transforms" a vector $\vec{x}$ into a new vector $\vec{y}$ via matrix-vector multiplication ($\vec{y} = A\vec{x}$). The matrix can scale, rotate, or shear space.
    \end{enumerate}
    
    \tcblower
    \textit{Storage:} Matrices are typically stored in row-major or column-major order in memory. \\
    \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(1)$
\end{tcolorbox}



\subsection{Types of Matrices}
\begin{tcolorbox}[breakable]
    \begin{enumerate}
        \item \textbf{Square Matrix:} $A \in \mathbb{R}^{n \times n}$ (number of rows = number of columns)
        \[
        A = \begin{bmatrix}
            1 & 2 & 3 \\
            4 & 5 & 6 \\
            7 & 8 & 9
        \end{bmatrix}_{3 \times 3}
        \]

        \item \textbf{Zero Matrix:} All entries are zero, denoted $O$ or $0_{m \times n}$
        \[
        O = \begin{bmatrix}
            0 & 0 & 0 \\
            0 & 0 & 0
        \end{bmatrix}_{2 \times 3}
        \]

        \item \textbf{Identity Matrix:} Square matrix with ones on the main diagonal and zeros elsewhere, denoted $I$ or $I_n$
        \[
        I_3 = \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{bmatrix}
        \]
        Formally: $(I_n)_{ij} = \delta_{ij} = \begin{cases} 1 & \text{if } i = j \\ 0 & \text{if } i \neq j \end{cases}$ (Kronecker delta function)



        \item \textbf{Diagonal Matrix:} Square matrix with non-zero entries only on the main diagonal
        \[
        D = \begin{bmatrix}
            d_1 & 0 & 0 \\
            0 & d_2 & 0 \\
            0 & 0 & d_3
        \end{bmatrix} = \text{diag}(d_1, d_2, d_3)
        \]
    
    
        \item \textbf{Upper Triangular Matrix:} All entries below the main diagonal are zero
        \[
        U = \begin{bmatrix}
            u_{11} & u_{12} & u_{13} \\
            0 & u_{22} & u_{23} \\
            0 & 0 & u_{33}
        \end{bmatrix}
        \]
        Formally: $u_{ij} = 0$ for $i > j$
    
        \item \textbf{Lower Triangular Matrix:} All entries above the main diagonal are zero
        \[
        L = \begin{bmatrix}
            l_{11} & 0 & 0 \\
            l_{21} & l_{22} & 0 \\
            l_{31} & l_{32} & l_{33}
        \end{bmatrix}
        \]
        Formally: $l_{ij} = 0$ for $i < j$

        \item \textbf{Symmetric Matrix:} Square matrix where $A = A^T$ (equal to its transpose)
        \[
        A = \begin{bmatrix}
            1 & 2 & 3 \\
            2 & 4 & 5 \\
            3 & 5 & 6
        \end{bmatrix}
        \]
        Formally: $a_{ij} = a_{ji}$ for all $i, j$
    
        \item \textbf{Skew-Symmetric Matrix:} Square matrix where $A = -A^T$
        \[
        A = \begin{bmatrix}
            0 & 2 & -3 \\
            -2 & 0 & 5 \\
            3 & -5 & 0
        \end{bmatrix}
        \]
        Formally: $a_{ij} = -a_{ji}$ for all $i, j$ (diagonal entries must be zero)
    
    
        \item \textbf{Orthogonal Matrix:} Square matrix where $A^TA = AA^T = I$ (columns and rows are orthonormal)
        
        \item \textbf{Row Echelon Form (REF):} Matrix where:
        \begin{itemize}
            \item All nonzero rows are above any rows of all zeros
            \item Leading entry (pivot) of each nonzero row is to the right of the leading entry of the row above it
        \end{itemize}

        \item \textbf{Reduced Row Echelon Form (RREF):} REF where:
        \begin{itemize}
            \item Each leading entry is 1
            \item Each leading 1 is the only nonzero entry in its column
        \end{itemize}
        
    \end{enumerate}

    \tcblower
    \textit{Note:} Special matrix types often have computational advantages (e.g., diagonal matrix multiplication is $O(n)$ instead of $O(n^3)$).
\end{tcolorbox}



\subsection{Matrix Operations}
\begin{tcolorbox}[breakable]
    For matrices $A, B \in \mathbb{R}^{m \times n}$, $\vec{x} \in \mathbb{R}^n$ and scalar $c \in \mathbb{R}$, the following rules hold::
    \begin{enumerate}
        \item \textbf{Scalar Multiplication:}
        \[ (c A)_{ij} = c(a_{ij}) \]
        \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mn)$.
        
        \item \textbf{Matrix Addition:}
        \[ (A + B)_{ij} = a_{ij} + b_{ij} \]
        \emph{Requirement:} Both matrices must have the same dimension. \\
        \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mn)$.
        
        \item \textbf{Matrix Subtraction:}
        \[ (A - B)_{ij} = a_{ij} - b_{ij} \]
        \emph{Requirement:} Both matrices must have the same dimension. \\
        \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mn)$.
        
        \item \textbf{Matrix Transpose:}
        \[ (A^T)_{ij} = a_{ji} \]
        \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mn)$.

        \item \textbf{Matrix-Vector Multiplication:}
        \[
        \vec{y} = A\vec{x} \quad (A \in \mathbb{R}^{m \times n}:  \vec{x} \in \mathbb{R}^{n} \mapsto \vec{y} \in \mathbb{R}^{m})
        \]
        \textit{Space Complexity}: $O(m)$ | \textit{Time Complexity}: $O(mn)$.

        \item \textbf{Matrix-Matrix Multiplication:}
        \[
        C = AB \quad (A \in \mathbb{R}^{m \times k}, B \in \mathbb{R}^{k \times n} \implies C \in \mathbb{R}^{m \times n})
        \]
        \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mnk)$. \\
        \textit{Note:} For two $n \times n$ matrices, the time complexity is $O(n^3)$.
    \end{enumerate}
\end{tcolorbox}



\subsection{Matrix-Vector Multiplication}
\begin{tcolorbox}
    The product of a matrix $A \in \mathbb{R}^{m \times n}$ and a column vector $\vec{x} \in \mathbb{R}^{n \times 1}$ results in a column vector $\vec{y} \in \mathbb{R}^{m \times 1}$ and is defined as:
    \[
    A\vec{x} = 
    \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix}
    \begin{bmatrix} 
        x_1 \\ x_2 \\ \vdots \\ x_n 
    \end{bmatrix} 
    = 
    \begin{bmatrix}
        a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n \\
        a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \\
        \vdots \\
        a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n
    \end{bmatrix}
    = \vec{y}.
    \]
    \emph{Requirement:} The number of columns in $A$ must equal the dimension of $\vec{x}$. \\
    This operation can be interpreted in two essential ways:

    \begin{enumerate}
        \item \textbf{The "Row Picture" (Dot Product Method):} \\ 
        Each entry of the output vector $\vec{y}$ is the \textbf{dot product} of the corresponding \textbf{row of $A$} with the vector $\vec{x}$.
        \[
        \begin{bmatrix}
            - & \vec{r}_1 & - \\
            - & \vec{r}_2 & - \\
            & \vdots & \\
            - & \vec{r}_m & -
        \end{bmatrix}
        \begin{bmatrix}
            | \\ \vec{x} \\ |
        \end{bmatrix}
        =
        \begin{bmatrix}
            \vec{r}_1 \cdot \vec{x} \\
            \vec{r}_2 \cdot \vec{x} \\
            \vdots \\
            \vec{r}_m \cdot \vec{x}
        \end{bmatrix}
        =
        \begin{bmatrix}
            y_1 \\ y_2 \\ \vdots \\ y_m
        \end{bmatrix}
        \]
        This is the standard method used for manual computation.
        
        \item \textbf{The "Column Picture" (Linear Combination Method):} \\
        The output vector $\vec{y}$ is a \textbf{linear combination} of the \textbf{columns of $A$}, where the \textbf{entries of $\vec{x}$ are the weights}.
        \[
        \begin{bmatrix}
        | & | & & | \\
        \vec{c}_1 & \vec{c}_2 & \cdots & \vec{c}_n \\
        | & | & & |
        \end{bmatrix}
        \begin{bmatrix}
        x_1 \\ x_2 \\ \vdots \\ x_n
        \end{bmatrix}
        = x_1 \begin{bmatrix} | \\ \vec{c}_1 \\ | \end{bmatrix} + x_2 \begin{bmatrix} | \\ \vec{c}_2 \\ | \end{bmatrix} + \cdots + x_n \begin{bmatrix} | \\ \vec{c}_n \\ | \end{bmatrix}
        \]
        This interpretation is more conceptual and is fundamental to understanding linear systems, column space, and rank.
    
    \end{enumerate}
    
    \tcblower
    \textit{Space Complexity}: $O(m)$ | \textit{Time Complexity}: $O(mn)$
\end{tcolorbox}



\subsection{Matrix-Matrix Multiplication}
\begin{tcolorbox}[breakable]
    For matrices $A \in \mathbb{R}^{m \times k}$ and $B \in \mathbb{R}^{k \times n}$, the product $C = AB \in \mathbb{R}^{m \times n}$ is defined as:
    \[
    C = AB =
    \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1k} \\
        a_{21} & a_{22} & \cdots & a_{2k} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mk}
    \end{bmatrix}
    \begin{bmatrix}
        b_{11} & b_{12} & \cdots & b_{1n} \\
        b_{21} & b_{22} & \cdots & b_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        b_{k1} & b_{k2} & \cdots & b_{kn}
    \end{bmatrix}
    =
    \begin{bmatrix}
        c_{11} & c_{12} & \cdots & c_{1n} \\
        c_{21} & c_{22} & \cdots & c_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        c_{m1} & c_{m2} & \cdots & c_{mn}
    \end{bmatrix}
    \]
    where each entry is computed as:
    \[
    c_{ij} = \sum_{r=1}^{k} a_{ir}b_{rj} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{ik}b_{kj}
    \]
    \emph{Requirement:} The number of columns in $A$ must equal the number of rows in $B$. \\
    This operation can be interpreted in multiple ways:
    
    \begin{enumerate}
        \item \textbf{The "Entry-wise" View (Dot Product Method):} \\
        Each entry $c_{ij}$ of the product matrix $C$ is the \textbf{dot product} of the $i$-th \textbf{row of $A$} with the $j$-th \textbf{column of $B$}.
        \[
        c_{ij} = \text{(row $i$ of $A$)} \cdot \text{(column $j$ of $B$)} = \sum_{r=1}^{k} a_{ir}b_{rj}
        \]
        \[
        \begin{bmatrix}
        \cdots & \vec{r}_i & \cdots
        \end{bmatrix}
        \begin{bmatrix}
        \vdots \\ \vec{c}_j \\ \vdots
        \end{bmatrix}
        =
        \begin{bmatrix}
        \ddots & \vdots & \\
        \cdots & C_{ij} & \cdots \\
         & \vdots & \ddots
        \end{bmatrix}
        \]
        This is the standard computational method.
        
        \item \textbf{The "Column Picture":} \\
        Each \textbf{column of $AB$} is a \textbf{linear combination of the columns of $A$}, with weights taken from the corresponding \textbf{column of} $B$.
        \[
        \text{col}_j(C) = A \cdot (\text{col}_j(B))
        \]
        \[
        AB = A
        \begin{bmatrix}
            | & | & & | \\
            \vec{b}_1 & \vec{b}_2 & \cdots & \vec{b}_n \\
            | & | & & |
        \end{bmatrix}
        =
        \begin{bmatrix}
            | & | & & | \\
            A\vec{b}_1 & A\vec{b}_2 & \cdots & A\vec{b}_n \\
            | & | & & |
        \end{bmatrix}
        \]
        This view shows that every column of $AB$ must be in the column space of $A$.
        
        \item \textbf{The "Row Picture":} \\
        Symmetrically, each \textbf{row of $AB$} is a \textbf{linear combination of the rows of $B$}, with weights taken from the corresponding \textbf{row of} $A$.
        \[
        \text{row}_i(C) = (\text{row}_i(A)) \cdot B
        \]
        \[
        AB = 
        \begin{bmatrix}
            - & \vec{a}_1 & - \\
            - & \vec{a}_2 & - \\
            & \vdots & \\
            - & \vec{a}_m & -
        \end{bmatrix}
        B
        =
        \begin{bmatrix}
            - & \vec{a}_1 B & - \\
            - & \vec{a}_2 B & - \\
            & \vdots & \\
            - & \vec{a}_m B & -
        \end{bmatrix}
        \]
        This view shows that every row of $C$ must be in the row space of $B$.
    \end{enumerate}
    
    \textbf{Identity Property:}
    \[
    AI_n = A, \quad I_mA = A
    \]
    where $I$ is the identity matrix of appropriate size.

    \tcblower
    \textit{Note:} Optimized algorithms like Strassen's can achieve $O(n^{2.807})$ for square matrices. Modern libraries use cache-optimized blocking and GPU parallelization for practical speedups. \\
    \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mnk)$ (naive algorithm), $O(n^3)$ (for $n \times n$ matrices).
\end{tcolorbox}



\subsection{Properties of Matrix Operations}
\begin{tcolorbox}[breakable]
    \textbf{Addition and Scalar Multiplication Properties:} Let $A, B, C \in \mathbb{R}^{m \times n}$ and $c, d \in \mathbb{R}$. Then:
    \[
    \begin{aligned}
        &\text{(1)} \quad A + B = B + A && \text{(Commutativity)} \\[8pt]
        &\text{(2)} \quad (A + B) + C = A + (B + C) && \text{(Associativity)} \\[8pt]
        &\text{(3)} \quad A + O = A && \text{(Additive Identity)} \\[8pt]
        &\text{(4)} \quad A + (-A) = O && \text{(Additive Inverse)} \\[8pt]
        &\text{(5)} \quad c(A + B) = cA + cB && \text{(Distributivity over Matrices)} \\[8pt]
        &\text{(6)} \quad (c + d)A = cA + dA && \text{(Distributivity over Scalars)} \\[8pt]
        &\text{(7)} \quad c(dA) = (cd)A && \text{(Associativity)} \\[8pt]
        &\text{(8)} \quad 1 \cdot A = A && \text{(Scalar Identity)}
    \end{aligned}
    \]
    
    \textbf{Multiplication Properties:} Let $A \in \mathbb{R}^{m \times n}$, $B \in \mathbb{R}^{n \times p}$, $C \in \mathbb{R}^{p \times q}$, and $c \in \mathbb{R}$. Then:
    \[
    \begin{aligned}
        &\text{(1)} \quad (AB)C = A(BC) && \text{(Associativity)} \\[8pt]
        &\text{(2)} \quad A(B + C) = AB + AC && \text{(Left Distributivity)} \\[8pt]
        &\text{(3)} \quad (A + B)C = AC + BC && \text{(Right DistributDivity)} \\[8pt]
        &\text{(4)} \quad c(AB) = (cA)B = A(cB) && \text{(Scalar Associativity)} \\[8pt]
        &\text{(5)} \quad AI_n = A \text{ and } I_mA = A && \text{(Multiplicative Identity)} \\[8pt]
        &\text{(6)} \quad AO = O = OA && \text{(Zero Product)}
    \end{aligned}
    \]
    
    \textbf{Warning:} Matrix multiplication is \textbf{not commutative} in general.
    \[
    AB \neq BA
    \]
    
    \tcblower
    Time: $O(mn)$ for addition/scalar multiplication, $O(mnp)$ for multiplication | Space: Result size. \\
    \textit{Computational Note:}
    \begin{itemize}
        \item \textbf{Associativity of Multiplication:} $O(mnk + mkl)$ vs $O(nkl + mnl)$. While $(AB)C$ and $A(BC)$ are mathematically equal, their computational cost can be vastly different. Choosing the optimal order is a key optimization problem (e.g., in deep learning backpropagation).
        \item \textbf{Complexity:} Verifying additive properties is $O(mn)$. Verifying multiplicative properties is $O(n^3)$ (for $n \times n$ matrices).
    \end{itemize}
\end{tcolorbox}



\subsection{Matrix Transpose}
\begin{tcolorbox}
    The \textbf{transpose} of a matrix $A \in \mathbb{R}^{m \times n}$ is the matrix $A^T \in \mathbb{R}^{n \times m}$ obtained by interchanging rows and columns:
    \[
    (A^T)_{ij} = a_{ji}
    \]
    \[
    A = \begin{bmatrix} - & \vec{r}_1 & - \\ - & \vec{r}_2 & - \\ & \vdots & \end{bmatrix} \implies A^T = \begin{bmatrix} | & | & & | \\ \vec{r}_1^T & \vec{r}_2^T & \cdots & \\ | & | & & | \end{bmatrix}.
    \]
    That is, the $i$-th row of $A$ becomes the $i$-th column of $A^T$. \\
    
    
    \textbf{For vectors:} A column vector becomes a row vector, and vice versa:
    \[
    \vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} \quad \implies \quad \vec{v}^T = \begin{bmatrix} v_1 & v_2 & v_3 \end{bmatrix}
    \]
    This is why $\vec{u} \cdot \vec{v} = \vec{u}^T\vec{v}$ (dot product as matrix multiplication).

    \textbf{Special Cases:}
    \[
    \begin{aligned}
        &\text{(1)} \quad A = A^T \iff A && \text{(Symmetric)} \\[8pt]
        &\text{(2)} \quad A = -A^T \iff A && \text{(Skew-symmetric)} \\[8pt]
        &\text{(3)} \quad (A^T)^T = A && \text{(Double transpose)} \\[8pt]
        &\text{(4)} \quad \vec{u} \cdot \vec{v} = \vec{u}^T\vec{v} && \text{(Dot Product as Matrix Multiplication)}
    \end{aligned}
    \]
    
    \tcblower
    \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mn)$.
\end{tcolorbox}



\subsection{Properties of Transpose}
\begin{tcolorbox}[breakable]
    Let $A, B$ be matrices of compatible dimensions and $c \in \mathbb{R}$. Then:
    \[
    \begin{aligned}
        &\text{(1)} \quad (A^T)^T = A && \text{(Double Transpose/Involution)} \\[8pt]
        &\text{(2)} \quad (A + B)^T = A^T + B^T && \text{(Additivity)} \\[8pt]
        &\text{(3)} \quad (cA)^T = cA^T && \text{(Scalar Multiplication)} \\[8pt]
        &\text{(4)} \quad (AB)^T = B^TA^T && \text{(Reversal Rule)} \\[8pt]
        &\text{(5)} \quad (A_1A_2 \cdots A_k)^T = A_k^T \cdots A_2^T A_1^T && \text{(Extended Reversal)} \\[8pt]
        &\text{(6)} \quad (\vec{u}^T\vec{v})^T = \vec{v}^T\vec{u} = \vec{u}^T\vec{v} && \text{(Dot Product Symmetry)}
    \end{aligned}
    \]
    \textbf{Transpose of the Identity Matrix:}
    \[
    I^T = I
    \]
    \textbf{Transpose Preserves Rank:}
    \[
    \operatorname{rank}(A^T) = \operatorname{rank}(A)
    \]
    \textbf{Transpose and Inverse:}
    If $A$ is invertible, then $A^T$ is also invertible, and:
    \[
    (A^T)^{-1} = (A^{-1})^T
    \]
\end{tcolorbox}



\subsection{Symmetric and Skew-Symmetric Matrices}
\begin{tcolorbox}[breakable]
    These are special types of square matrices defined by their relationship with their own transpose. \\
    
    \textbf{Symmetric Matrix:} A square matrix $A \in \mathbb{R}^{n \times n}$ is \textbf{symmetric} if it is equal to its transpose.
    \[
    A \text{ is symmetric} \iff A = A^T \iff a_{ij} = a_{ji} \quad \forall i, j
    \]
    
    \textbf{Skew-Symmetric Matrix:} A square matrix $A \in \mathbb{R}^{n \times n}$ is \textbf{skew-symmetric} (or \textbf{anti-symmetric}) if it is equal to the negative of its transpose
    \[
    A \text{ is skew-symmetric} \iff A = -A^T \iff a_{ij} = -a_{ji} \quad \forall i, j
    \]
    Note: This implies $a_{ii} = 0$ (diagonal entries must be zero). \\

    \textbf{Decomposition Theorem:} Any square matrix $A \in \mathbb{R}^{n \times n}$ can be \textbf{uniquely} decomposed as:
    \[
    A = \underbrace{\frac{A + A^T}{2}}_{\text{Symmetric part } S} + \underbrace{\frac{A - A^T}{2}}_{\text{Skew-symmetric part } K}
    \]
    
    \textbf{Key Properties:}
    \begin{enumerate}
        \item A symmetric matrix must be square
        \item A skew-symmetric matrix must have zeros on the diagonal
        \item $A^TA$ is always symmetric (Gram matrix)
        \item $AA^T$ is always symmetric (Covariance structure)
        \item $\vec{x}^TA\vec{x} = \vec{x}^TA^T\vec{x}$ for symmetric $A$ (Quadratic form)
        \item Any square matrix $A$ can be uniquely expressed as the sum of a symmetric matrix $S$ and a skew-symmetric matrix $K$:
        \[
        A = S + K
        \]
        \item If $A$ and $B$ are symmetric and commute ($AB = BA$), then $AB$ is symmetric
        \item The eigenvalues of a real symmetric matrix are real
        \item The eigenvalues of a real skew-symmetric matrix are purely imaginary or zero
    \end{enumerate}

    Tip: Symmetric = Scaling, Skew-symmetric = Rotation. \\

    \textbf{Special Property for Skew-Symmetric:}
    \[
    \det(K) = 0 \text{ if } n \text{ is odd}, \quad \det(K) \geq 0 \text{ if } n \text{ is even}
    \]
    
    \tcblower
    \textit{Space Complexity}: $\frac{n(n+1)}{2}$ | \textit{Time Complexity}: $O(n^2)$.
\end{tcolorbox}



\subsection{Trace of a Matrix}
\begin{tcolorbox}[breakable]
    The \textbf{trace} of a square matrix $A \in \mathbb{R}^{n \times n}$, denoted $\text{tr}(A)$, is the \textbf{sum of its diagonal entries}:
    \[
    \text{tr}(A) = \sum_{i=1}^{n} a_{ii} = a_{11} + a_{22} + \cdots + a_{nn}
    \]
    
    \textbf{Example:}
    \[
    A = \begin{bmatrix}
        \mathbf{1} & 2 & 3 \\
        4 & \mathbf{5} & 6 \\
        7 & 8 & \mathbf{9}
    \end{bmatrix} \implies \text{tr}(A) = 1 + 5 + 9 = 15
    \]
    
    \textbf{Important Note:} Trace is \textbf{only defined for square matrices}. For non-square matrices, the concept is undefined. \\

    \textbf{Properties:} Let $A, B \in \mathbb{R}^{n \times n}$ and $c \in \mathbb{R}$
    \[
    \begin{aligned}
        &\text{(1)} \quad \text{tr}(A + B) = \text{tr}(A) + \text{tr}(B) && \text{(Additivity)} \\[8pt]
        &\text{(2)} \quad \text{tr}(cA) = c \cdot \text{tr}(A) && \text{(Homogeneity)} \\[8pt]
        &\text{(3)} \quad \text{tr}(A^T) = \text{tr}(A) && \text{(Transpose Invariance)} \\[8pt]
        &\text{(4)} \quad \text{tr}(AB) = \text{tr}(BA) && \text{(Cyclic Property)} \\[8pt]
        &\text{(5)} \quad \text{tr}(ABC) = \text{tr}(CAB) = \text{tr}(BCA) && \text{(Extended Cyclic)} \\[8pt]
        &\text{(6)} \quad \text{tr}(I_n) = n && \text{(Identity Trace)} \\[8pt]
        &\text{(7)} \quad \text{tr}(\vec{u}\vec{v}^T) = \vec{u}^T\vec{v} = \vec{v}^T\vec{u} && \text{(Outer Product)}
    \end{aligned}
    \]
    
    Properties (1) and (2) show that trace is a \textbf{linear functional} (or linear operator) on the space of matrices. \\
    \textit{Note on (4):} Even though $AB \neq BA$ in general, their traces are equal. \\

    \textbf{Special Identities:}
    \begin{enumerate}
        \item $\text{tr}(A^TA) = \text{tr}(AA^T) = \sum_{i,j} a_{ij}^2 = \|A\|_F^2$ (Frobenius norm squared/Matrix magnitude)
        \item $\text{tr}(\vec{x}\vec{x}^T) = \vec{x}^T\vec{x} = \|\vec{x}\|^2$ (Vector norm)
    \end{enumerate}
    
    \textbf{Relationship to Eigenvalues:} For a matrix $A$ with eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_n$:
    \[
    \text{tr}(A) = \sum_{i=1}^{n} \lambda_i
    \]
    This is a fundamental result connecting diagonal entries to spectral properties (covered in detail in eigenvalue section).
    
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$.
\end{tcolorbox}



\subsection{Relationship Between Trace and Eigenvalues}
\begin{tcolorbox}[breakable]
    For any square matrix $A \in \mathbb{R}^{n \times n}$, the \textbf{trace} of $A$ is equal to the \textbf{sum of all its eigenvalues} ($\lambda_1, \lambda_2, \dots, \lambda_n$):
    
    \[
    \text{tr}(A) = \sum_{i=1}^{n} \lambda_i = \lambda_1 + \lambda_2 + \cdots + \lambda_n
    \]
    This holds \textbf{regardless of whether $A$ is diagonalizable}.
    
    \textit{Notes:}
    \begin{itemize}
        \item The eigenvalues $\lambda_i$ are the roots of the characteristic polynomial of $A$.
        \item This property holds even if the eigenvalues are repeated (counted with their algebraic multiplicity) or are complex numbers (for $A \in \mathbb{C}^{n \times n}$).
    \end{itemize}
    
    \textbf{Companion Result (Determinant):}
    \[
    \det(A) = \prod_{i=1}^{n} \lambda_i = \lambda_1 \cdot \lambda_2 \cdots \lambda_n
    \]
    
    Together, these provide powerful invariants of a matrix under similarity transformations.
    
    \textbf{Proof via Characteristic Polynomial:}
    
    The characteristic polynomial of $A$ is:
    \[
    p(\lambda) = \det(\lambda I - A) = \lambda^n + c_{n-1}\lambda^{n-1} + \cdots + c_1\lambda + c_0
    \]
    
    By the fundamental theorem of algebra, this factors as:
    \[
    p(\lambda) = (\lambda - \lambda_1)(\lambda - \lambda_2) \cdots (\lambda - \lambda_n)
    \]
    
    Expanding and comparing coefficients:
    \[
    \begin{aligned}
        &\text{Coefficient of } \lambda^{n-1}: \quad c_{n-1} = -(\lambda_1 + \lambda_2 + \cdots + \lambda_n) \\[8pt]
        &\text{Constant term}: \quad c_0 = (-1)^n \lambda_1 \lambda_2 \cdots \lambda_n = (-1)^n \det(A)
    \end{aligned}
    \]
    
    From the definition of characteristic polynomial, $c_{n-1} = -\text{tr}(A)$, thus:
    \[
    \text{tr}(A) = \sum_{i=1}^{n} \lambda_i
    \]
    
    \textbf{Key Properties:}
    \[
    \begin{aligned}
        &\text{(1)} \quad \text{tr}(A^k) = \sum_{i=1}^{n} \lambda_i^k && \text{(Powers of eigenvalues)} \\[8pt]
        &\text{(2)} \quad \text{tr}(P^{-1}AP) = \text{tr}(A) && \text{(Similarity invariance)} \\[8pt]
        &\text{(3)} \quad \text{tr}(e^A) = \sum_{i=1}^{n} e^{\lambda_i} && \text{(Matrix exponential)} \\[8pt]
        &\text{(4)} \quad \text{tr}(A^{-1}) = \sum_{i=1}^{n} \lambda_i^{-1} && \text{(Inverse, if exists)} \\[8pt]
        &\text{(5)} \quad \text{tr}(A) = 0 \centernot\implies \lambda_i = 0 && \text{(Zero trace doesn't imply zero eigenvalues)}
    \end{aligned}
    \]
    
    Property (2) is crucial: similar matrices (representing the same linear transformation in different bases) have the same trace and eigenvalues.
    
    \textbf{Example:}
    \[
    A = \begin{bmatrix}
        2 & 1 \\
        1 & 2
    \end{bmatrix}
    \]
    Eigenvalues: $\lambda_1 = 3, \lambda_2 = 1$ (verify: $\det(\lambda I - A) = (\lambda - 3)(\lambda - 1) = 0$)
    \[
    \text{tr}(A) = 2 + 2 = 4 = 3 + 1 = \lambda_1 + \lambda_2 \quad \checkmark
    \]
    \[
    \det(A) = 4 - 1 = 3 = 3 \cdot 1 = \lambda_1 \lambda_2 \quad \checkmark
    \]
    
    \textbf{Geometric Interpretation:}
    \begin{itemize}
        \item \textbf{Trace:} Measures total "stretching" along all eigendirections. Sum of scaling factors.
        \item \textbf{Determinant:} Measures volume distortion. Product of scaling factors.
        \item For rotation matrices (orthogonal), eigenvalues lie on unit circle: $|\lambda_i| = 1$, so $\text{tr}(Q) \leq n$ with equality for identity.
    \end{itemize}
    
    
    \textbf{Special Cases:}
    \[
    \begin{aligned}
        &\text{Diagonal matrix:} \quad D = \text{diag}(d_1, \ldots, d_n) \implies \lambda_i = d_i, \quad \text{tr}(D) = \sum d_i \\[8pt]
        &\text{Triangular matrix:} \quad \text{Eigenvalues are diagonal entries} \\[8pt]
        &\text{Symmetric matrix:} \quad \text{All eigenvalues real, orthogonal eigenvectors} \\[8pt]
        &\text{Orthogonal matrix:} \quad |\lambda_i| = 1, \quad \text{tr}(Q) = \sum \lambda_i \text{ (complex sum)}
    \end{aligned}
    \]
\end{tcolorbox}



\subsection{Powers of Matrices}
\begin{tcolorbox}[breakable]
    For a square matrix $A \in \mathbb{R}^{n \times n}$ and a non-negative integer $k$, the \textbf{power} $A^k$ is defined as the matrix $A$ multiplied by itself $k$ times:
    
    \textbf{Definition:}
    \[
    A^k = \underbrace{A \cdot A \cdot \ldots \cdot A}_{k \text{ times}}
    \]
    
    \textbf{Recursive Definition:}
    \[
    A^k = A \cdot A^{k-1} \quad \text{for } k \geq 1
    \]
    
    \emph{Requirement:} Powers are only defined for \textbf{square matrices}, as the dimensions must be compatible for repeated multiplication ($n \times n$ times $n \times n$).
    
    \textbf{Special Cases:}
    \[
    \begin{aligned}
        &\text{(1)} \quad A^0 = I && \text{(Identity, by convention)} \\[8pt]
        &\text{(2)} \quad A^1 = A && \text{(First power)} \\[8pt]
        &\text{(3)} \quad A^{-k} = (A^{-1})^k = (A^k)^{-1} && \text{(Negative powers, if $A$ invertible)} \\[8pt]
        &\text{(4)} \quad (cA)^k = c^k A^k && \text{(Scalar multiplication)}
    \end{aligned}
    \]
    
    \textbf{Fundamental Properties:}
    \[
    \begin{aligned}
        &\text{(1)} \quad A^m A^n = A^{m+n} && \text{(Exponent addition)} \\[8pt]
        &\text{(2)} \quad (A^m)^n = A^{mn} && \text{(Exponent multiplication)} \\[8pt]
        &\text{(3)} \quad (A^T)^k = (A^k)^T && \text{(Transpose commutes)} \\[8pt]
        &\text{(4)} \quad (A^{-1})^k = (A^k)^{-1} && \text{(Inverse commutes)} \\[8pt]
        &\text{(5)} \quad AB = BA \implies (AB)^k = A^k B^k && \text{(Commuting matrices only)} \\[8pt]
        &\text{(6)} \quad \text{tr}(A^k) = \sum_{i=1}^{n} \lambda_i^k && \text{(Trace of power)}
    \end{aligned}
    \]
    
    \textbf{Warning:} In general, $(AB)^k \neq A^k B^k$ unless $AB = BA$.
    
    \textbf{Computational Methods:}
    
    \begin{enumerate}
        \item \textbf{Naive Repeated Multiplication:} 
        $$A^k = A \cdot A \cdot A \cdots A \quad (k-1 \text{ multiplications}) - O(kn^3)$$

        \item \textbf{Binary Exponentiation (Repeated Squaring):} 
        $$A^{13} = A^8 \cdot A^4 \cdot A^1 - O(n^3 \log k)$$
        
        \item \textbf{Diagonalization Method (Best for multiple powers):} 
        $$\textstyle A^k = (PDP^{-1})^k = PD^kP^{-1} - O(n^3)$$
    \end{enumerate}
    
    \tcblower
    \textit{Space Complexity}: $O(n^2)$ | \textit{Time Complexity}: $O(kn^3)\ |\ O(n^3 \log k)\ |\ O(n^3)$.
\end{tcolorbox}



\subsection{Elementary Matrices}
\begin{tcolorbox}[breakable]

    An \textbf{elementary matrix} ($E$) is a matrix is obtained by performing a \textbf{single elementary row operation} on the identity matrix ($I$). \\

    \textbf{Three Types of Elementary Matrices:}
    
    \begin{enumerate}
        \item \textbf{Type I — Row Swap:} $E_{ij}$ swaps rows $i$ and $j$
        \[
        I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \xrightarrow{R_1 \leftrightarrow R_2} E_{12} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \quad \text{(swaps rows 1 and 2)}
        \]
        
        \item \textbf{Type II — Row Scaling:} $E_i(c)$ multiplies row $i$ by non-zero scalar $c$
        \[
        I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \xrightarrow{5R_1 \to R_1} E_2(5) = \begin{bmatrix} 1 & 0 \\ 0 & 5 \end{bmatrix} \quad \text{(scales row 2 by 5)}
        \]
        
        \item \textbf{Type III — Row Addition:} $E_{ij}(c)$ adds $c$ times row $j$ to row $i$
        \[
        I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \xrightarrow{R_2 + 4R_1 \to R_2} E_{21}(4) = \begin{bmatrix} 1 & 0 \\ 4 & 1 \end{bmatrix} \quad \text{(adds 4 times row 1 to row 2)}
        \]
    \end{enumerate}
    
    \textbf{Key Principle:} Performing elementary row operation on matrix $A$ is equivalent to \textbf{left-multiplying} $A$ by the corresponding elementary matrix:
    \[
    \text{Row operation on } A \iff EA
    \]
    where $E$ is the elementary matrix representing that operation.
    
    \textbf{Fundamental Properties:}
    \[
    \begin{aligned}
        &\text{(1)} \quad \text{Every elementary matrix is \textbf{invertible}} && \\[8pt]
        &\text{(2)} \quad \text{The inverse of an elementary matrix is also elementary} && \\[8pt]
        &\text{(3)} \quad \det(E) \neq 0 \text{ for all elementary matrices} && \text{(Always invertible)} \\[8pt]
        &\text{(4)} \quad (EA)^T = A^T E^T && \text{(Transpose relation)}
    \end{aligned}
    \]
    
    \textbf{Inverses of Elementary Matrices:}
    \[
    \begin{aligned}
        &\text{Type I:} \quad E_{ij}^{-1} = E_{ij} && \text{(Self-inverse: swap twice returns original)} \\[8pt]
        &\text{Type II:} \quad E_i(c)^{-1} = E_i(1/c) && \text{(Scale by reciprocal)} \\[8pt]
        &\text{Type III:} \quad E_{ij}(c)^{-1} = E_{ij}(-c) && \text{(Negate the coefficient)}
    \end{aligned}
    \]
    
    \textbf{Determinants of Elementary Matrices:}
    \[
    \begin{aligned}
        &\text{Type I:} \quad \det(E_{ij}) = -1 && \text{(Row swap changes sign)} \\[8pt]
        &\text{Type II:} \quad \det(E_i(c)) = c && \text{(Scaling factor)} \\[8pt]
        &\text{Type III:} \quad \det(E_{ij}(c)) = 1 && \text{(No change)}
    \end{aligned}
    \]
    These correspond exactly to how elementary row operations affect determinants! \\
    
    \textbf{Geometric Interpretation:}
    \begin{itemize}
        \item \textbf{Type I:} Reflection across a hyperplane (swaps two basis vectors)
        \item \textbf{Type II:} Scaling along one coordinate axis
        \item \textbf{Type III:} Shear transformation (preserves volume, hence $\det = 1$)
    \end{itemize}
    
    Every invertible linear transformation can be decomposed into these three basic operations!
\end{tcolorbox}



\subsection{Block Matrices}
\begin{tcolorbox}[breakable]
    A \textbf{block matrix} (or \textbf{partitioned matrix}) is a matrix divided into submatrices called \textbf{blocks}:
    \[
    M = \begin{bmatrix}
        A & B \\
        C & D
    \end{bmatrix}
    \]
    where $A \in \mathbb{R}^{p \times q}$, $B \in \mathbb{R}^{p \times r}$, $C \in \mathbb{R}^{s \times q}$, $D \in \mathbb{R}^{s \times r}$, and $M \in \mathbb{R}^{(p+s) \times (q+r)}$.
    
    \textbf{General Form:}
    \[
    M = \begin{bmatrix}
        A_{11} & A_{12} & \cdots & A_{1n} \\
        A_{21} & A_{22} & \cdots & A_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        A_{m1} & A_{m2} & \cdots & A_{mn}
    \end{bmatrix}
    \]
    where each $A_{ij}$ is a submatrix (block).
    
    \textbf{Block Matrix Operations:}
    
    Operations on block matrices follow the same rules as scalar matrices, \textbf{provided dimensions are compatible}.
    
    \begin{enumerate}
        \item \textbf{Block Addition:}
        \[
        \begin{bmatrix} A & B \\ C & D \end{bmatrix} + \begin{bmatrix} E & F \\ G & H \end{bmatrix} = \begin{bmatrix} A+E & B+F \\ C+G & D+H \end{bmatrix}
        \]
        Requirement: Corresponding blocks must have same dimensions.
        
        \item \textbf{Block Scalar Multiplication:}
        \[
        c\begin{bmatrix} A & B \\ C & D \end{bmatrix} = \begin{bmatrix} cA & cB \\ cC & cD \end{bmatrix}
        \]
        
        \item \textbf{Block Matrix Multiplication:}
        \[
        \begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} E & F \\ G & H \end{bmatrix} = \begin{bmatrix} AE+BG & AF+BH \\ CE+DG & CF+DH \end{bmatrix}
        \]
        Requirement: Inner dimensions must match ($A \in \mathbb{R}^{p \times q}$, $E \in \mathbb{R}^{q \times r}$, etc.).
        
        This follows the same "row-times-column" pattern as scalar matrix multiplication!
        
        \item \textbf{Block Transpose:}
        \[
        \begin{bmatrix} A & B \\ C & D \end{bmatrix}^T = \begin{bmatrix} A^T & C^T \\ B^T & D^T \end{bmatrix}
        \]
        Blocks are transposed \textbf{and} positions are transposed.
    \end{enumerate}
\end{tcolorbox}

\end{document}
