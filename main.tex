\documentclass[a4paper,11pt]{article}

\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{hyperref}
\usepackage{datetime2}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{tcolorbox}
\tcbuselibrary{breakable, skins}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{centernot}


\newcommand{\startdate}{November 8, 2025}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


\title{Linear Algebra Notes}
\author{Abror Maksudov}
\date{Created: \startdate \\[7pt] Last Updated: \DTMnow}

\everymath{\displaystyle}

\begin{document}

\maketitle
\tableofcontents



\section{Vectors}



\subsection{Definition of a Scalar}
\begin{tcolorbox}
    A scalar is a \textbf{single number}, denoted as $a, b, c, \alpha, \beta, \gamma, \lambda, \mu$.
    
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}:  $N/A$.
\end{tcolorbox}



\subsection{Definition of a Vector}
\begin{tcolorbox}
    A vector is an \textbf{ordered list} of numbers, usually denoted as $\mathbf{v}$ or $\vec{v}$.

    \begin{itemize}
        \item \textbf{Column Vector (Default/Standard):}
        $$
        \vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix} = \begin{bmatrix} v_1 & v_2 & \cdots & v_n \end{bmatrix}^T
        $$

        \item \textbf{Row Vector:}
        $$
        \vec{v} =  \begin{bmatrix} v_1 & v_2 & \cdots & v_n \end{bmatrix} = \begin{pmatrix} v_1 & v_2 & \cdots & v_n \end{pmatrix} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}^T
        $$
    \end{itemize}

    \tcblower
    \textit{Space Complexity}: $O(n)$ | \textit{Time Complexity}:  $N/A$.
\end{tcolorbox}



\subsection{Vector Operations}
\begin{tcolorbox}
    For vectors $\vec{u}, \vec{v} \in \mathbb{R}^n$ and scalar $c \in \mathbb{R}$, the following rules hold:
    \begin{enumerate}
        \item \textbf{Scalar Multiplication:}
        $$
        c\vec{v} = c\begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = \begin{bmatrix} cv_1 \\ cv_2 \\ \vdots \\ cv_n \end{bmatrix}
        $$
        
        \item \textbf{Vector Addition:}
        $$
        \vec{u} + \vec{v} = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix} + \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix}
        $$
        \emph{Requirement:} Both vectors must have the same dimension.
        
        \item \textbf{Vector Subtraction:}
        $$
        \vec{u} - \vec{v} = \vec{u} + (-1)\vec{v} = \begin{bmatrix} u_1 - v_1 \\ u_2 - v_2 \\ \vdots \\ u_n - v_n \end{bmatrix}
        $$
        \emph{Requirement:} Both vectors must have the same dimension.
    \end{enumerate}
    
    \tcblower
    \textit{Space Complexity}: $O(n)$ | \textit{Time Complexity}:  $O(n)$.
\end{tcolorbox}



\subsection{Properties of Vector}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v}, \vec{w} \in \mathbb{R}^n$ and $c, d \in \mathbb{R}$. Then:
    \[
    \begin{aligned}
        &\text{(1)} \quad \vec{u} + \vec{v} = \vec{v} + \vec{u} && \text{(Commutativity)} \\[8pt]
        &\text{(2)} \quad (\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w}) && \text{(Associativity)} \\[8pt]
        &\text{(3)} \quad \vec{v} + \vec{0} = \vec{v} && \text{(Identity)} \\[8pt]
        &\text{(4)} \quad \vec{v} + (-\vec{v}) = \vec{0} && \text{(Inverse)} \\[8pt]
        &\text{(5)} \quad c(d\vec{v}) = (cd)\vec{v} && \text{(Associativity of scalars)} \\[8pt]
        &\text{(6)} \quad 1 \cdot \vec{v} = \vec{v} && \text{(Multiplicative identity)} \\[8pt]
        &\text{(7)} \quad c(\vec{u} + \vec{v}) = c\vec{u} + c\vec{v} && \text{(Distributivity over vectors)} \\[8pt]
        &\text{(8)} \quad (c + d)\vec{v} = c\vec{v} + d\vec{v} && \text{(Distributivity over scalars)}
    \end{aligned}
    \]
    These 8 axioms define a linear space (or vector space) in $\mathbb{R}^n.$
\end{tcolorbox}



\subsection{Linear Combinations}
\begin{tcolorbox}
    A vector $\vec{w} \in \mathbb{R}^n$ is a \textbf{linear combination} of vectors $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k \in \mathbb{R}^n$ if there exist scalars $c_1, c_2, \ldots, c_k \in \mathbb{R}$ such that:
    \[
    \vec{w} = c_1\vec{v}_1 + c_2\vec{v}_2 + \cdots + c_k\vec{v}_k = \sum_{i=1}^{k} c_i\vec{v}_i = \begin{bmatrix} | & | & & | \\ \vec{v}_1 & \vec{v}_2 & \cdots & \vec{v}_k \\ | & | & & | \end{bmatrix} \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_k \end{bmatrix} = \vec{w}
    \]
    
    The scalars $c_1, c_2, \ldots, c_k$ are called \textbf{coefficients} or \textbf{weights} of the linear combination.
    
    \tcblower
    \textit{Space Complexity}: $O(n)$ | \textit{Time Complexity}: $O(nk)$.
\end{tcolorbox}



\subsection{Dot Product}

\begin{tcolorbox}
    The \textbf{dot product} of two vectors $\vec{u}, \vec{v} \in \mathbb{R}^n$ is a scalar defined as:
    \[
    \vec{u} \cdot \vec{v} = u_1v_1 + u_2v_2 + \cdots + u_nv_n = \sum_{i=1}^{n} u_iv_i
    \]

    \textbf{Geometric View:}
    \[
    \vec{u} \cdot \vec{v} = \|\vec{u}\| \|\vec{v}\| \cos\theta
    \]
    where $\theta$ is the angle between $\vec{u}$ and $\vec{v}$, and $\|\vec{u}\|$ is the \textbf{magnitude} (or \textbf{length} or \textbf{norm}) of $\vec{u}$. \\
    
    \textbf{Alternative notations:}
    \[
    \vec{u} \cdot \vec{v} = \langle \vec{u}, \vec{v} \rangle = \mathbf{u}^T\mathbf{v}
    \]
    
    \textbf{Special Cases:}
    \[
    \begin{aligned}
        &\text{(1)} \quad \vec{u} \cdot \vec{v} = 0 \iff \theta = 90^\circ && \text{(Perpendicular)} \\[8pt]
        &\text{(2)} \quad \vec{u} \cdot \vec{v} > 0 \iff \theta < 90^\circ && \text{(Acute angle)} \\[8pt]
        &\text{(3)} \quad \vec{u} \cdot \vec{v} < 0 \iff \theta > 90^\circ && \text{(Obtuse angle)} \\[8pt]
        &\text{(4)} \quad \vec{u} \cdot \vec{u} = \|\vec{u}\|^2 \iff \theta = 0^\circ && \text{(Squared magnitude)} \\[8pt]
    \end{aligned}
    \]
    
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$. \\
    \textit{Note:} Highly optimized using SIMD (Single Instruction, Multiple Data) operations.
\end{tcolorbox}



\subsection{Properties of the Dot Product}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v}, \vec{w} \in \mathbb{R}^n$ and $c \in \mathbb{R}$. Then:
    \[
    \begin{aligned}
        &\text{(1)} \quad \vec{u} \cdot \vec{v} = \vec{v} \cdot \vec{u} && \text{(Commutativity)} \\[8pt]
        &\text{(2)} \quad \vec{u} \cdot (\vec{v} + \vec{w}) = \vec{u} \cdot \vec{v} + \vec{u} \cdot \vec{w} && \text{(Distributivity)} \\[8pt]
        &\text{(3)} \quad (c\vec{u}) \cdot \vec{v} = c(\vec{u} \cdot \vec{v}) = \vec{u} \cdot (c\vec{v}) && \text{(Scalar multiplication)} \\[8pt]
        &\text{(4)} \quad \vec{u} \cdot \vec{u} \geq 0 && \text{(Positive definiteness)} \\[8pt]
        &\text{(5)} \quad \vec{u} \cdot \vec{u} = 0 \iff \vec{u} = \vec{0} && \text{(Definiteness)} \\[8pt]
        &\text{(6)} \quad \vec{0} \cdot \vec{v} = 0 && \text{(Zero vector)} \\[8pt]
    \end{aligned}
    \]

    These properties define the dot product as an \textbf{inner product} on $\mathbb{R}^n$.
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$. \\
\end{tcolorbox}



\subsection{Cross Product}
\begin{tcolorbox}
    The \textbf{cross product} of two vectors $\vec{u}, \vec{v} \in \mathbb{R}^3$ is a vector $\vec{u} \times \vec{v} \in \mathbb{R}^3$ that is \textbf{perpendicular} to both $\vec{u}$ and $\vec{v}$ as is defined as:
    \[
    \vec{u} \times \vec{v} = \begin{bmatrix} u_2v_3 - u_3v_2 \\ u_3v_1 - u_1v_3 \\ u_1v_2 - u_2v_1 \end{bmatrix}
    \]
    
    \tcblower
    \textit{Note:} Cross product is only defined in $\mathbb{R}^3$ (and $\mathbb{R}^7$, but rarely used). \\
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(1)$
\end{tcolorbox}



\subsection{Properties of the Cross Product}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v}, \vec{w} \in \mathbb{R}^3$, $c \in \mathbb{R}$ and $\theta$ be the angle between $\vec{u}$ and $\vec{v}$. Then:
    \[
    \begin{aligned}
        &\text{(1)} \quad \vec{u} \times \vec{v} = -(\vec{v} \times \vec{u}) && \text{(Anti-commutativity)} \\[8pt]
        &\text{(2)} \quad \vec{u} \times (\vec{v} + \vec{w}) = \vec{u} \times \vec{v} + \vec{u} \times \vec{w} && \text{(Distributivity)} \\[8pt]
        &\text{(3)} \quad (c\vec{u}) \times \vec{v} = c(\vec{u} \times \vec{v}) = \vec{u} \times (c\vec{v}) && \text{(Scalar multiplication)} \\[8pt]
        &\text{(4)} \quad \vec{u} \times \vec{u} = \vec{0} && \text{(Self cross product)} \\[8pt]
        &\text{(5)} \quad \vec{u} \times \vec{0} = \vec{0} \times \vec{u} = \vec{0} && \text{(Zero vector)} \\[8pt]
        &\text{(6)} \quad \vec{u} \times \vec{v} = \vec{0} \iff \vec{u} \parallel \vec{v} && \text{(Parallel vectors)} \\[8pt]
        &\text{(7)} \quad \vec{u} \cdot (\vec{u} \times \vec{v}) = 0, \quad \vec{v} \cdot (\vec{u} \times \vec{v}) = 0 && \text{(Orthogonality)} \\[8pt]
        &\text{(8)} \quad \|\vec{u} \times \vec{v}\| = \|\vec{u}\| \|\vec{v}\| \sin\theta && \text{(Magnitude)} \\[8pt]
    \end{aligned}
    \]

    \textbf{Warning:} The cross product is \textbf{not associative} in general: $\vec{u} \times (\vec{v} \times \vec{w}) \neq (\vec{u} \times \vec{v}) \times \vec{w}$.
    
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(1)$
\end{tcolorbox}



\subsection{Scalar Projection}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v} \in\mathbb{R}^n$. Then, the \textbf{scalar (component) projection} of $\vec{u}$ onto $\vec{v}$ is the signed length of the projection:

    \[
    \text{comp}_{\vec{v}} \vec{u} = \frac{\vec{u} \cdot \vec{v}}{\|\vec{v}\|}
    \]

    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$.
\end{tcolorbox}




\subsection{Vector Projection}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v} \in\mathbb{R}^n$. Then, the \textbf{vector projection} of $\vec{u}$ onto $\vec{v}$ is a vector in the direction of $\vec{v}$:
    \[
    \text{proj}_{\vec{v}} \vec{u} = \frac{\vec{u} \cdot \vec{v}}{\|\vec{v}\|^2} \vec{v} = \frac{\vec{u} \cdot \vec{v}}{\vec{v} \cdot \vec{v}} \vec{v} = \left(\text{comp}_{\vec{v}} \vec{u}\right) \frac{\vec{v}}{\|\vec{v}\|}
    \]

    \textbf{Orthogonal Component:}
    
    The component of $\vec{u}$ orthogonal to $\vec{v}$ is:
    \[
    \vec{u}_{\perp} = \vec{u} - \text{proj}_{\vec{v}} \vec{u}
    \]
    
    Note that $\vec{u} = \text{proj}_{\vec{v}} \vec{u} + \vec{u}_{\perp}$ (orthogonal decomposition).
    
    \tcblower
    \textit{Space Complexity}: $O(n)$ | \textit{Time Complexity}: $O(n)$.
\end{tcolorbox}



\subsection{Angle Between Vectors}
\begin{tcolorbox}
    The \textbf{angle} $\theta$ between two non-zero vectors $\vec{u}, \vec{v} \in \mathbb{R}^n$ is given by:
    \[
    \cos\theta = \frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\| \|\vec{v}\|} \iff     \theta = \arccos\left(\frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\| \|\vec{v}\|}\right), \quad 0 \leq \theta \leq \pi
    \]

    \textbf{Special Cases:}
    \[
    \begin{aligned}
        &\text{(1)} \quad \theta = 0 \iff \vec{u} \text{ and } \vec{v} \text{ point in the same direction} && (\cos\theta = 1) \\[8pt]
        &\text{(2)} \quad \theta = \frac{\pi}{2} \iff \vec{u} \perp \vec{v} && (\cos\theta = 0) \\[8pt]
        &\text{(3)} \quad \theta = \pi \iff \vec{u} \text{ and } \vec{v} \text{ point in opposite directions} && (\cos\theta = -1) \\[8pt]
    \end{aligned}
    \]
    
    \tcblower
    \textit{Note:} The formula $\vec{u} \cdot \vec{v} = \|\vec{u}\| \|\vec{v}\| \cos\theta$ is the geometric definition of the dot product. \\
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$
\end{tcolorbox}



\subsection{Cauchy-Schwarz Inequality}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v} \in \mathbb{R}^n$. Then:
    \[
    |\langle \vec{u}, \vec{v}\rangle|^2 \leq \langle\vec{u}, \vec{u}\rangle \cdot \langle\vec{v}, \vec{v}\rangle,
    \]
    or,
    \[
    |\langle \vec{u}, \vec{v}\rangle|^2 \leq \norm{\vec{u}}^2 \norm{\vec{v}}^2 \iff |\langle \vec{u}, \vec{v}\rangle| =\norm{\vec{u}} \norm{\vec{v}} |\cos\theta| \leq \norm{\vec{u}} \norm{\vec{v}},
    \]
    or in component form,
    \[
    \left(\sum_{i=1}^{n} u_iv_i\right)^2 \leq \left(\sum_{i=1}^{n} u_i^2\right)\left(\sum_{i=1}^{n} v_i^2\right).
    \]
    Equality holds if and only if $\vec{u}$ and $\vec{v}$ are linearly dependent.
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$
\end{tcolorbox}



\subsection{Triangle Inequality}
\begin{tcolorbox}
    Let $\vec{u}, \vec{v} \in \mathbb{R}^n$. Then:
    \[
    \|\vec{u} + \vec{v}\| \leq \|\vec{u}\| + \|\vec{v}\|
    \]
    Equality holds if and only if $\vec{u} = c\vec{v}$ with $c \geq 0$. \\
    \textbf{Reverse Triangle Inequality:}
    \[
    \big| \|\vec{u}\| - \|\vec{v}\| \big| \leq \|\vec{u} - \vec{v}\|
    \]
    
    \tcblower
    \textit{Space Complexity}: $O(1)$ | \textit{Time Complexity}: $O(n)$
\end{tcolorbox}



\subsection{Vectorization for Computational Efficiency}
\begin{tcolorbox}
    \textbf{Vectorization} is the process of replacing explicit loops with vector/matrix operations to achieve dramatic performance improvements by leveraging optimized, low-level linear algebra libraries and hardware parallelism. \\
    
    \textbf{Performance Example:}
    \begin{center}
    \begin{tabular}{l|c|c|c}
        \hline
        \textbf{Operation} & \textbf{Complexity} & \textbf{Loop (Python)} & \textbf{Vectorized (NumPy)} \\
        \hline
        Dot Product ($n=10^6$) & $O(n)$ & $\sim 500$ ms & $\sim 1$ ms \\
        Matrix-Vector ($n \times n$) & $O(n^2)$ & $\sim 2000$ ms & $\sim 10$ ms \\
        Matrix-Matrix ($n \times n$) & $O(n^3)$ & $\sim$ hours & $\sim$ seconds \\
        \hline
    \end{tabular}
    \end{center}
    
    \textbf{Why Vectorization is Faster:}
    \begin{enumerate}
        \item \textbf{Hardware (SIMD):} Modern CPUs use \textbf{S}ingle \textbf{I}nstruction, \textbf{M}ultiple \textbf{D}ata (SIMD) instructions. This allows a single CPU instruction to perform the same operation (e.g., multiplication) on multiple data elements (e.g., 8 or 16 numbers) at the same time. Vectorized code uses these instructions automatically; Python loops do not.

        \item \textbf{Cache Efficiency:} Vectorized operations access memory in large, contiguous blocks. This is very "cache-friendly" and avoids the high cost of fetching data from main memory (RAM) for each small step of a loop.
        
        \item \textbf{Optimized Libraries:} NumPy, PyTorch, and TensorFlow operations are thin wrappers around highly optimized libraries (like BLAS/LAPACK, MKL, cuBLAS) written in C or Fortran. These libraries have been fine-tuned for decades.
    \end{enumerate}

    \textbf{Space-Time Trade-off:}
    \begin{itemize}
        \item Loops: $O(1)$ extra space, slow
        \item Vectorized: May need $O(n)$ temporary arrays, fast
        \item \textit{Rule:} Memory is cheap, time is expensive
    \end{itemize}
\end{tcolorbox}



\section{Matrices}



\subsection{Definition of a Matrix}
\begin{tcolorbox}[breakable]
    A matrix is a \textbf{rectangular array} of numbers arranged in rows and columns, denoted as $A, B, C$ or $\mathbf{A}, \mathbf{B}, \mathbf{C}$. \\
    
    An $m \times n$ matrix $A$ has $m$ rows and $n$ columns:
    \[
    A = 
    \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix}
    =
    \begin{pmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix} 
    = [a_{ij}]_{m \times n}
    \]
    
    where $a_{ij}$ is the element in the $i$-th row and $j$-th column also denoted as $A_{ij}$. \\

    \textbf{Notation:}
    \begin{itemize}
        \item $A \in \mathbb{R}^{m \times n}$ means $A$ is an $m \times n$ matrix with real entries
        \item $(A)_{ij} = a_{ij}$ denotes the $(i,j)$-entry of matrix $A$
        \item $m \times n$ is the \textbf{dimension} or \textbf{size} of the matrix
    \end{itemize}
    
    \textbf{Special Cases:}
    \[
    \begin{aligned}
        &\text{(1)} \quad m = n \implies A \text{ is a \textbf{square matrix}} \quad (n \times n) \\[8pt]
        &\text{(2)} \quad m = 1 \implies A \text{ is a \textbf{row vector}} \quad (1 \times n) \\[8pt]
        &\text{(3)} \quad n = 1 \implies A \text{ is a \textbf{column vector}} \quad (m \times 1) \\[8pt]
        &\text{(4)} \quad m = n = 1 \implies A \text{ is a \textbf{scalar}} \quad (1 \times 1)\\[8pt]
    \end{aligned}
    \]

    \textbf{Interpretations:}
    Unlike vectors, matrices have multiple common interpretations:
    
    \begin{enumerate}
        \item \textbf{As Data:} A way to store data in a grid.
        
        \item \textbf{As a Collection of Vectors:}
        \[
        \begin{array}{c@{\hspace{2cm}}c}
        \begin{bmatrix}
        | & | & & | \\
        \vec{c}_1 & \vec{c}_2 & \cdots & \vec{c}_n \\
        | & | & & |
        \end{bmatrix}
        &
        \begin{bmatrix}
        - & \vec{r}_1 & - \\
        - & \vec{r}_2 & - \\
        & \vdots & \\
        - & \vec{r}_m & -
        \end{bmatrix}
        \\
        \rule{0pt}{15pt}\text{Set of column vectors} &
        \rule{0pt}{15pt}\text{Set of row vectors}
        \end{array}
        \]
        
        \item \textbf{As a Linear Transformation:} A function that "transforms" a vector $\vec{x}$ into a new vector $\vec{y}$ via matrix-vector multiplication ($\vec{y} = A\vec{x}$). The matrix can scale, rotate, or shear space.
    \end{enumerate}
    
    \tcblower
    \textit{Storage:} Matrices are typically stored in row-major or column-major order in memory. \\
    \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(1)$
\end{tcolorbox}



\subsection{Types of Matrices}
\begin{tcolorbox}[breakable]
    \begin{enumerate}
        \item \textbf{Square Matrix:} $A \in \mathbb{R}^{n \times n}$ (number of rows = number of columns)
        \[
        A = \begin{bmatrix}
            1 & 2 & 3 \\
            4 & 5 & 6 \\
            7 & 8 & 9
        \end{bmatrix}_{3 \times 3}
        \]

        \item \textbf{Zero Matrix:} All entries are zero, denoted $O$ or $0_{m \times n}$
        \[
        O = \begin{bmatrix}
            0 & 0 & 0 \\
            0 & 0 & 0
        \end{bmatrix}_{2 \times 3}
        \]

        \item \textbf{Identity Matrix:} Square matrix with ones on the main diagonal and zeros elsewhere, denoted $I$ or $I_n$
        \[
        I_3 = \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{bmatrix}
        \]
        Formally: $(I_n)_{ij} = \delta_{ij} = \begin{cases} 1 & \text{if } i = j \\ 0 & \text{if } i \neq j \end{cases}$ (Kronecker delta function)



        \item \textbf{Diagonal Matrix:} Square matrix with non-zero entries only on the main diagonal
        \[
        D = \begin{bmatrix}
            d_1 & 0 & 0 \\
            0 & d_2 & 0 \\
            0 & 0 & d_3
        \end{bmatrix} = \text{diag}(d_1, d_2, d_3)
        \]
    
    
        \item \textbf{Upper Triangular Matrix:} All entries below the main diagonal are zero
        \[
        U = \begin{bmatrix}
            u_{11} & u_{12} & u_{13} \\
            0 & u_{22} & u_{23} \\
            0 & 0 & u_{33}
        \end{bmatrix}
        \]
        Formally: $u_{ij} = 0$ for $i > j$
    
        \item \textbf{Lower Triangular Matrix:} All entries above the main diagonal are zero
        \[
        L = \begin{bmatrix}
            l_{11} & 0 & 0 \\
            l_{21} & l_{22} & 0 \\
            l_{31} & l_{32} & l_{33}
        \end{bmatrix}
        \]
        Formally: $l_{ij} = 0$ for $i < j$

        \item \textbf{Symmetric Matrix:} Square matrix where $A = A^T$ (equal to its transpose)
        \[
        A = \begin{bmatrix}
            1 & 2 & 3 \\
            2 & 4 & 5 \\
            3 & 5 & 6
        \end{bmatrix}
        \]
        Formally: $a_{ij} = a_{ji}$ for all $i, j$
    
        \item \textbf{Skew-Symmetric Matrix:} Square matrix where $A = -A^T$
        \[
        A = \begin{bmatrix}
            0 & 2 & -3 \\
            -2 & 0 & 5 \\
            3 & -5 & 0
        \end{bmatrix}
        \]
        Formally: $a_{ij} = -a_{ji}$ for all $i, j$ (diagonal entries must be zero)
    
    
        \item \textbf{Orthogonal Matrix:} Square matrix where $A^TA = AA^T = I$ (columns and rows are orthonormal)
        
        \item \textbf{Row Echelon Form (REF):} Matrix where:
        \begin{itemize}
            \item All nonzero rows are above any rows of all zeros
            \item Leading entry (pivot) of each nonzero row is to the right of the leading entry of the row above it
        \end{itemize}

        \item \textbf{Reduced Row Echelon Form (RREF):} REF where:
        \begin{itemize}
            \item Each leading entry is 1
            \item Each leading 1 is the only nonzero entry in its column
        \end{itemize}
        
    \end{enumerate}

    \tcblower
    \textit{Note:} Special matrix types often have computational advantages (e.g., diagonal matrix multiplication is $O(n)$ instead of $O(n^3)$).
\end{tcolorbox}



\subsection{Matrix Operations}
\begin{tcolorbox}[breakable]
    For matrices $A, B \in \mathbb{R}^{m \times n}$, $\vec{x} \in \mathbb{R}^n$ and scalar $c \in \mathbb{R}$, the following rules hold::
    \begin{enumerate}
        \item \textbf{Scalar Multiplication:}
        \[ (c A)_{ij} = c(a_{ij}) \]
        \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mn)$.
        
        \item \textbf{Matrix Addition:}
        \[ (A + B)_{ij} = a_{ij} + b_{ij} \]
        \emph{Requirement:} Both matrices must have the same dimension. \\
        \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mn)$.
        
        \item \textbf{Matrix Subtraction:}
        \[ (A - B)_{ij} = a_{ij} - b_{ij} \]
        \emph{Requirement:} Both matrices must have the same dimension. \\
        \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mn)$.
        
        \item \textbf{Matrix Transpose:}
        \[ (A^T)_{ij} = a_{ji} \]
        \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mn)$.

        \item \textbf{Matrix-Vector Multiplication:}
        \[
        \vec{y} = A\vec{x} \quad (A \in \mathbb{R}^{m \times n}:  \vec{x} \in \mathbb{R}^{n} \mapsto \vec{y} \in \mathbb{R}^{m})
        \]
        \textit{Space Complexity}: $O(m)$ | \textit{Time Complexity}: $O(mn)$.

        \item \textbf{Matrix-Matrix Multiplication:}
        \[
        C = AB \quad (A \in \mathbb{R}^{m \times k}, B \in \mathbb{R}^{k \times n} \implies C \in \mathbb{R}^{m \times n})
        \]
        \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mnk)$. \\
        \textit{Note:} For two $n \times n$ matrices, the time complexity is $O(n^3)$.
    \end{enumerate}
\end{tcolorbox}



\subsection{Matrix-Vector Multiplication}
\begin{tcolorbox}
    The product of a matrix $A \in \mathbb{R}^{m \times n}$ and a column vector $\vec{x} \in \mathbb{R}^{n \times 1}$ results in a column vector $\vec{y} \in \mathbb{R}^{m \times 1}$ and is defined as:
    \[
    A\vec{x} = 
    \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix}
    \begin{bmatrix} 
        x_1 \\ x_2 \\ \vdots \\ x_n 
    \end{bmatrix} 
    = 
    \begin{bmatrix}
        a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n \\
        a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \\
        \vdots \\
        a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n
    \end{bmatrix}
    = \vec{y}.
    \]
    \emph{Requirement:} The number of columns in $A$ must equal the dimension of $\vec{x}$. \\
    This operation can be interpreted in two essential ways:

    \begin{enumerate}
        \item \textbf{The "Row Picture" (Dot Product Method):} \\ 
        Each entry of the output vector $\vec{y}$ is the \textbf{dot product} of the corresponding \textbf{row of $A$} with the vector $\vec{x}$.
        \[
        \begin{bmatrix}
            - & \vec{r}_1 & - \\
            - & \vec{r}_2 & - \\
            & \vdots & \\
            - & \vec{r}_m & -
        \end{bmatrix}
        \begin{bmatrix}
            | \\ \vec{x} \\ |
        \end{bmatrix}
        =
        \begin{bmatrix}
            \vec{r}_1 \cdot \vec{x} \\
            \vec{r}_2 \cdot \vec{x} \\
            \vdots \\
            \vec{r}_m \cdot \vec{x}
        \end{bmatrix}
        =
        \begin{bmatrix}
            y_1 \\ y_2 \\ \vdots \\ y_m
        \end{bmatrix}
        \]
        This is the standard method used for manual computation.
        
        \item \textbf{The "Column Picture" (Linear Combination Method):} \\
        The output vector $\vec{y}$ is a \textbf{linear combination} of the \textbf{columns of $A$}, where the \textbf{entries of $\vec{x}$ are the weights}.
        \[
        \begin{bmatrix}
        | & | & & | \\
        \vec{c}_1 & \vec{c}_2 & \cdots & \vec{c}_n \\
        | & | & & |
        \end{bmatrix}
        \begin{bmatrix}
        x_1 \\ x_2 \\ \vdots \\ x_n
        \end{bmatrix}
        = x_1 \begin{bmatrix} | \\ \vec{c}_1 \\ | \end{bmatrix} + x_2 \begin{bmatrix} | \\ \vec{c}_2 \\ | \end{bmatrix} + \cdots + x_n \begin{bmatrix} | \\ \vec{c}_n \\ | \end{bmatrix}
        \]
        This interpretation is more conceptual and is fundamental to understanding linear systems, column space, and rank.
    
    \end{enumerate}
    
    \tcblower
    \textit{Space Complexity}: $O(m)$ | \textit{Time Complexity}: $O(mn)$
\end{tcolorbox}



\subsection{Matrix-Matrix Multiplication}
\begin{tcolorbox}[breakable]
    For matrices $A \in \mathbb{R}^{m \times k}$ and $B \in \mathbb{R}^{k \times n}$, the product $C = AB \in \mathbb{R}^{m \times n}$ is defined as:
    \[
    C = AB =
    \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1k} \\
        a_{21} & a_{22} & \cdots & a_{2k} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mk}
    \end{bmatrix}
    \begin{bmatrix}
        b_{11} & b_{12} & \cdots & b_{1n} \\
        b_{21} & b_{22} & \cdots & b_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        b_{k1} & b_{k2} & \cdots & b_{kn}
    \end{bmatrix}
    =
    \begin{bmatrix}
        c_{11} & c_{12} & \cdots & c_{1n} \\
        c_{21} & c_{22} & \cdots & c_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        c_{m1} & c_{m2} & \cdots & c_{mn}
    \end{bmatrix}
    \]
    where each entry is computed as:
    \[
    c_{ij} = \sum_{r=1}^{k} a_{ir}b_{rj} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{ik}b_{kj}
    \]
    \emph{Requirement:} The number of columns in $A$ must equal the number of rows in $B$. \\
    This operation can be interpreted in multiple ways:
    
    \begin{enumerate}
        \item \textbf{The "Entry-wise" View (Dot Product Method):} \\
        Each entry $c_{ij}$ of the product matrix $C$ is the \textbf{dot product} of the $i$-th \textbf{row of $A$} with the $j$-th \textbf{column of $B$}.
        \[
        c_{ij} = \text{(row $i$ of $A$)} \cdot \text{(column $j$ of $B$)} = \sum_{r=1}^{k} a_{ir}b_{rj}
        \]
        \[
        \begin{bmatrix}
        \cdots & \vec{r}_i & \cdots
        \end{bmatrix}
        \begin{bmatrix}
        \vdots \\ \vec{c}_j \\ \vdots
        \end{bmatrix}
        =
        \begin{bmatrix}
        \ddots & \vdots & \\
        \cdots & C_{ij} & \cdots \\
         & \vdots & \ddots
        \end{bmatrix}
        \]
        This is the standard computational method.
        
        \item \textbf{The "Column Picture":} \\
        Each \textbf{column of $AB$} is a \textbf{linear combination of the columns of $A$}, with weights taken from the corresponding \textbf{column of} $B$.
        \[
        \text{col}_j(C) = A \cdot (\text{col}_j(B))
        \]
        \[
        AB = A
        \begin{bmatrix}
            | & | & & | \\
            \vec{b}_1 & \vec{b}_2 & \cdots & \vec{b}_n \\
            | & | & & |
        \end{bmatrix}
        =
        \begin{bmatrix}
            | & | & & | \\
            A\vec{b}_1 & A\vec{b}_2 & \cdots & A\vec{b}_n \\
            | & | & & |
        \end{bmatrix}
        \]
        This view shows that every column of $AB$ must be in the column space of $A$.
        
        \item \textbf{The "Row Picture":} \\
        Symmetrically, each \textbf{row of $AB$} is a \textbf{linear combination of the rows of $B$}, with weights taken from the corresponding \textbf{row of} $A$.
        \[
        \text{row}_i(C) = (\text{row}_i(A)) \cdot B
        \]
        \[
        AB = 
        \begin{bmatrix}
            - & \vec{a}_1 & - \\
            - & \vec{a}_2 & - \\
            & \vdots & \\
            - & \vec{a}_m & -
        \end{bmatrix}
        B
        =
        \begin{bmatrix}
            - & \vec{a}_1 B & - \\
            - & \vec{a}_2 B & - \\
            & \vdots & \\
            - & \vec{a}_m B & -
        \end{bmatrix}
        \]
        This view shows that every row of $C$ must be in the row space of $B$.
    \end{enumerate}
    
    \textbf{Identity Property:}
    \[
    AI_n = A, \quad I_mA = A
    \]
    where $I$ is the identity matrix of appropriate size.

    \tcblower
    \textit{Note:} Optimized algorithms like Strassen's can achieve $O(n^{2.807})$ for square matrices. Modern libraries use cache-optimized blocking and GPU parallelization for practical speedups. \\
    \textit{Space Complexity}: $O(mn)$ | \textit{Time Complexity}: $O(mnk)$ (naive algorithm), $O(n^3)$ (for $n \times n$ matrices).
\end{tcolorbox}

\end{document}
